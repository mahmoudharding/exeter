{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 08: Python String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:15.363920Z",
     "start_time": "2018-02-02T15:15:14.337886Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonicalization with Basic Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_state = pd.read_csv('data/county_and_state.csv')\n",
    "county_and_pop = pd.read_csv('data/county_and_population.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we'd like to join these two tables. Unfortunately, we can't, because the strings representing the county names don't match, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>De Witt County</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lac qui Parle County</td>\n",
       "      <td>MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lewis and Clark County</td>\n",
       "      <td>MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>St John the Baptist Parish</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       County State\n",
       "0              De Witt County    IL\n",
       "1        Lac qui Parle County    MN\n",
       "2      Lewis and Clark County    MT\n",
       "3  St John the Baptist Parish    LS"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_and_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeWitt</td>\n",
       "      <td>16798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lac Qui Parle</td>\n",
       "      <td>8067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lewis &amp; Clark</td>\n",
       "      <td>55716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>St. John the Baptist</td>\n",
       "      <td>43044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 County  Population\n",
       "0                DeWitt       16798\n",
       "1         Lac Qui Parle        8067\n",
       "2         Lewis & Clark       55716\n",
       "3  St. John the Baptist       43044"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_and_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before we can join them, we'll do what I call **canonicalization**.\n",
    "\n",
    "[Canonicalization](https://en.wikipedia.org/wiki/Canonicalization): A process for converting data that has more than one possible representation into a \"standard\", \"normal\", or canonical form (definition via Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop['clean_county'] = county_and_pop['County'].map(canonicalize_county)\n",
    "county_and_state['clean_county'] = county_and_state['County'].map(canonicalize_county)\n",
    "\n",
    "display(county_and_pop)  # display outputs even if not last line in cell - like a fancy print()\n",
    "county_and_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop.merge(county_and_state, on='clean_county')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data from a Text Log Using Basic Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fname = 'data/log.txt'\n",
    "!cat {log_fname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(log_fname, 'r') as f:\n",
    "    log_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to extract the day, month, year, hour, minutes, seconds, and timezone. Looking at the data, we see that these items are not in a fixed position relative to the beginning of the string. That is, slicing by some fixed offset isn't going to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[0][20:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[1][20:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll need to use some more sophisticated thinking. Let's focus on only the first line of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = log_lines[0]\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertinent = first.split(\"[\")[1].split(']')[0]\n",
    "day, month, rest = pertinent.split('/')\n",
    "year, hour, minute, rest = rest.split(':')\n",
    "seconds, time_zone = rest.split(' ')\n",
    "day, month, year, hour, minute, seconds, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much more sophisticated but common approach is to extract the information we need using a regular expression. See [today's lecture slides](https://ds100.org/sp22/lecture/lec06/) (Spring 2022) for more on regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization with Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python `re.sub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<div><td valign=\"top\">Moo</td></div>'\n",
    "pattern = r\"<[^>]+>\"\n",
    "re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "`pandas`: `Series.str.replace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html = pd.DataFrame(['<div><td valign=\"top\">Moo</td></div>',\n",
    "                   '<a href=\"http://ds100.org\">Link</a>',\n",
    "                   '<b>Bold text</b>'], columns=['Html'])\n",
    "df_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series -> Series\n",
    "df_html[\"Html\"].str.replace(pattern, '', regex=True).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Extraction with Regex\n",
    "\n",
    "Python `re.findall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My social security number is 123-45-6789 bro, or actually maybe itâ€™s 321-45-6789.\";\n",
    "pattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\n",
    "re.findall(pattern, text)  # ['123-45-6789', '321-45-6789']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Observations: 03:04:53 - Horse awakens.\n",
    "03:05:14 - Horse goes back to sleep.\"\"\"       \n",
    "pattern = r\"(\\d\\d):(\\d\\d):(\\d\\d) - (.*)\"\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "`pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn = pd.DataFrame(\n",
    "    ['987-65-4321',\n",
    "     'forty',\n",
    "     '123-45-6789 bro or 321-45-6789',\n",
    "     '999-99-9999'],\n",
    "    columns=['SSN'])\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Series.str.findall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Series of lists\n",
    "pattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\n",
    "df_ssn['SSN'].str.findall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `Series.str.extract`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> DataFrame of first match group\n",
    "pattern_group = r\"([0-9]{3}-[0-9]{2}-[0-9]{4})\" # 1 group\n",
    "df_ssn['SSN'].str.extract(pattern_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract first match of all groups\n",
    "pattern_group_mult = r\"([0-9]{3})-([0-9]{2})-([0-9]{4})\" # 3 groups\n",
    "df_ssn['SSN'].str.extract(pattern_group_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `Series.str.extractall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> DataFrame, one row per match\n",
    "df_ssn['SSN'].str.extractall(pattern_group_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original dataframe\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting Text Log Processing using Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = log_lines[0]\n",
    "display(line)\n",
    "\n",
    "pattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "day, month, year, hour, minute, second, time_zone = re.findall(pattern, line)[0] # get first match\n",
    "day, month, year, hour, minute, second, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions can be compiled and used as an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx = re.compile(pattern)\n",
    "rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx.search(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rx.search(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rx.search(line)\n",
    "out.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets you write conditional code more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [line, \"blah blah blah\"]\n",
    "for l in inputs:\n",
    "    out = rx.search(l)\n",
    "    if out:\n",
    "        print(out.group(0))\n",
    "    else:\n",
    "        print(f'*** No match for: {l[0:5]} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beyond the scope of lecture, but left here for your interest\n",
    "day, month, year, hour, minute, second, time_zone = re.search(pattern, line).groups()\n",
    "day, month, year, hour, minute, second, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### Pandas version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(log_lines, columns=['Log'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: `Series.str.findall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "df['Log'].str.findall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Option 2: `Series.str.extractall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log'].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling either of these two DataFrames into a nice format (like below) is left as an exercise for you! You will do a related problem on the homework.\n",
    "\n",
    "\n",
    "||Day|Month|Year|Hour|Minute|Second|Time Zone|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|0|26|Jan|2014|10|47|58|-0800|\n",
    "|1|2|Feb|2005|17|23|6|-0800|\n",
    "|2|3|Feb|2006|10|18|37|-0800|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Real World Example #1: Restaurant Data\n",
    "\n",
    "In this example, we will show how regexes can allow us to track quantitative data across categories defined by the appearance of various text fields.\n",
    "\n",
    "In this example we'll see how the presence of certain keywords can affect quantitative data:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio = pd.read_csv('data/violations.csv', header=0, names=['bid', 'date', 'desc'])\n",
    "desc = vio['desc']\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = desc.value_counts()\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of different descriptions!! Can we **canonicalize** at all? Let's explore two sets of 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hmmm...\n",
    "counts[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to cut out the extra info in square braces.\n",
    "vio['clean_desc'] = (vio['desc']\n",
    "             .str.replace(r'\\s*\\[.*\\]$', '', regex=True)\n",
    "             .str.strip()       # removes leading/trailing whitespace\n",
    "             .str.lower())\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonicalizing definitely helped\n",
    "vio['clean_desc'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['clean_desc'].value_counts().head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our research question:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Below, we use regular expressions and `df.assign()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html?highlight=assign#pandas.DataFrame.assign)) to **method chain** our creation of new boolean features, one per keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to assign new features for the presence of various keywords\n",
    "# regex metacharacter | \n",
    "with_features = (vio\n",
    " .assign(is_unclean     = vio['clean_desc'].str.contains('clean|sanit'))\n",
    " .assign(is_high_risk = vio['clean_desc'].str.contains('high risk'))\n",
    " .assign(is_vermin    = vio['clean_desc'].str.contains('vermin'))\n",
    " .assign(is_surface   = vio['clean_desc'].str.contains('wall|ceiling|floor|surface'))\n",
    " .assign(is_human     = vio['clean_desc'].str.contains('hand|glove|hair|nail'))\n",
    " .assign(is_permit    = vio['clean_desc'].str.contains('permit|certif'))\n",
    ")\n",
    "with_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "### EDA\n",
    "\n",
    "That's the end of our text wrangling. Now let's do some more analysis to analyze restaurant health as a function of the number of violation keywords.\n",
    "\n",
    "To do so we'll first group so that our **granularity** is one inspection for a business on particular date. This effectively counts the number of violations by keyword for a given inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features = (with_features\n",
    " .groupby(['bid', 'date'])\n",
    " .sum()\n",
    " .reset_index()\n",
    ")\n",
    "count_features.iloc[255:260, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out our new dataframe in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features.query('is_vermin > 1').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll reshape this \"wide\" table into a \"tidy\" table using a pandas feature called `pd.melt` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html?highlight=pd%20melt)) which we won't describe in any detail, other than that it's effectively the inverse of `pd.pivot_table`.\n",
    "\n",
    "Our **granularity** is now a violation type for a given inspection (for a business on a particular date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_down_by_violation_type = pd.melt(count_features, id_vars=['bid', 'date'],\n",
    "            var_name='feature', value_name='num_vios')\n",
    "\n",
    "# show a particular inspection's results\n",
    "broken_down_by_violation_type.query('bid == 489 & date == 20150728')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our research question:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)\n",
    "\n",
    "<br/>\n",
    "\n",
    "We have the second half of this question! Now let's **join** our table with the inspection scores, located in `inspections.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the scores\n",
    "ins = pd.read_csv('data/inspections.csv',\n",
    "                  header=0,\n",
    "                  usecols=[0, 1, 2],\n",
    "                  names=['bid', 'score', 'date'])\n",
    "ins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the inspection scores were stored in a separate file from the violation descriptions, we notice that the **primary key** in inspections is (`bid`, `date`)! So we can reference this key in our join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join scores with the table broken down by violation type\n",
    "violation_type_and_scores = (\n",
    "    broken_down_by_violation_type\n",
    "    .merge(ins, on=['bid', 'date'])\n",
    ")\n",
    "violation_type_and_scores.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "Let's plot the distribution of scores, broken down by violation counts, for each inspection feature (`is_clean`, `is_high_risk`, `is_vermin`, `is_surface`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will learn this syntax next week. Focus on interpreting for now.\n",
    "sns.catplot(x='num_vios', y='score',\n",
    "               col='feature', col_wrap=2,\n",
    "               kind='box',\n",
    "               data=violation_type_and_scores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can observe:\n",
    "* The inspection score generally goes down with increasing numbers of violations, as expected.\n",
    "* Depending on the violation keyword, inspections scores on average go down at slightly different rates.\n",
    "* For example, that if a restaurant inspection involved 2 violations with the keyword \"vermin\", the average score for that inspection would be a little bit below 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Content: Using pd.to_datetime to Extract Time Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date parsing using `pd.to_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(log_lines).str.extract(r'\\[(.*) -0800\\]').apply(\n",
    "    lambda s: pd.to_datetime(s, format='%d/%b/%Y:%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
